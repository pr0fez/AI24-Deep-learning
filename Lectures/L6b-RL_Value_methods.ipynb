{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e2c3334",
   "metadata": {},
   "source": [
    "### Deep Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb3b3b9",
   "metadata": {},
   "source": [
    "Värdebaserade metoder lär sig förutsäga värden istället för att direkt optimera på en loss-funktion. I de flesta fall används ett _Q värde_, där Q antyder \"quality\". I allmänhet innebär det att vi måste ha någon sorts data att först lära oss Q-värdena från. En fortfarande använd, men ineffektiv, metod är att bygga upp ett stickprov genom att observera utfallet av slumpmässiga handlingar under någon viss tid. Alternativ kan förstås en sådan _erfarenhetsbuffer_ förberedas med kända exempel, som \"alla partier turneringsschack spelade sedan 1886\". \n",
    "\n",
    "Under ytan handlar det egentligen om en statistisk modell över _stokastiska processer_ även kända som _Markov Decision Processes_ (MDP). Se kapitel 19 i boken för mer detaljer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6daacf7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "max_episode_steps=1000\n",
    "def show_one_episode(policy, seed=42):\n",
    "    frames = []\n",
    "    env = gym.make(\"CartPole-v1\", render_mode=\"human\",\n",
    "                   max_episode_steps=max_episode_steps)\n",
    "    obs, info = env.reset(seed=seed)\n",
    "    while True:\n",
    "        frames.append(env.render())\n",
    "        action = policy(obs)\n",
    "        obs, reward, done, truncated, info = env.step(action)\n",
    "        if done or truncated:\n",
    "            print(f\"Failed? {done}\")\n",
    "            break        \n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b1fbea",
   "metadata": {},
   "source": [
    "DQN nätverket lär sig förutsäga Q-värdena givet ett visst tillstånd. För att välja en handling tar vi helt enkelt den med högst förutsagt värde. 'Epsilon' värdet, en hyperparameter för metoden, anger sannolikheten att vi tar en slumpmässig handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "cc0d7465",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.Linear(4, 32), nn.ReLU(),\n",
    "                                 nn.Linear(32, 32), nn.ReLU(),\n",
    "                                 nn.Linear(32, 2))\n",
    "\n",
    "    def forward(self, state):\n",
    "        return self.net(state)\n",
    "\n",
    "def choose_dqn_action(model, obs, epsilon=0.0):\n",
    "        if torch.rand(()) < epsilon:  # epsilon greedy policy\n",
    "            return torch.randint(2, size=()).item()\n",
    "        else:\n",
    "            state = torch.as_tensor(obs)\n",
    "            Q_values = model(state)\n",
    "            return Q_values.argmax().item()  # optimal according to the DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9bc2fe",
   "metadata": {},
   "source": [
    "Givet en _replay buffer_ tar vi slumpmässiga stickprov från den, <code>batch_size</code> långa. Buffern är ordnad över tid, så det är bara startpunkten som är slumpmässig-- stickprovet är en serie tillstånd med tillhörande handlingar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "27cc88dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_experiences(replay_buffer, batch_size):\n",
    "    indices = torch.randint(len(replay_buffer), size=[batch_size])\n",
    "    batch = [replay_buffer[index] for index in indices.tolist()]\n",
    "    return [to_tensor([exp[index] for exp in batch]) for index in range(6)]\n",
    "\n",
    "def to_tensor(data):\n",
    "    array = np.stack(data)\n",
    "    dtype = torch.float32 if array.dtype == np.float64 else None\n",
    "    return torch.as_tensor(array, dtype=dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a492bc",
   "metadata": {},
   "source": [
    "Ett sätt att implementera en fix buffer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c4a15429",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, max_length):\n",
    "        self.data = [None] * max_length\n",
    "        self.max_length = max_length\n",
    "        self.index = 0\n",
    "        self.length = 0\n",
    "\n",
    "    def append(self, obj):\n",
    "        self.data[self.index] = obj\n",
    "        self.length = min(self.length + 1, self.max_length)\n",
    "        self.index = (self.index + 1) % self.max_length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if index < 0 or index >= self.length:\n",
    "            raise IndexError(f\"replay buffer index out of range: {index}\")\n",
    "        return self.data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0cb5c0b",
   "metadata": {},
   "source": [
    "För att observera en episod i en omgivning för en viss policy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d952b85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_and_record_episode(model, env, replay_buffer, epsilon, seed=None):\n",
    "    obs, _info = env.reset(seed=seed)\n",
    "    total_rewards = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        while True:\n",
    "            action = choose_dqn_action(model, obs, epsilon)\n",
    "            next_obs, reward, done, truncated, _info = env.step(action)\n",
    "            experience = (obs, action, reward, next_obs, done, truncated)\n",
    "            replay_buffer.append(experience)\n",
    "            total_rewards += reward\n",
    "            if done or truncated:\n",
    "                return total_rewards\n",
    "            obs = next_obs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9ba6a9",
   "metadata": {},
   "source": [
    "Ett träningssteg:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df34518d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'criterion' är loss-funktionen\n",
    "def dqn_training_step(model, optimizer, criterion, replay_buffer, batch_size,\n",
    "                      discount_factor):\n",
    "    experiences = sample_experiences(replay_buffer, batch_size)\n",
    "    state, action, reward, next_state, done, truncated = experiences\n",
    "    with torch.inference_mode():\n",
    "        next_Q_value = model(next_state)\n",
    "\n",
    "    max_next_Q_value, _ = next_Q_value.max(dim=1)\n",
    "    running = (~(done | truncated)).float()  # 0 slut, 1 pågående\n",
    "    # vad vi 'borde' få, enligt nätverket:\n",
    "    target_Q_value = reward + running * discount_factor * max_next_Q_value\n",
    "    all_Q_values = model(state)\n",
    "    Q_value = all_Q_values.gather(dim=1, index=action.unsqueeze(1))\n",
    "    loss = criterion(Q_value, target_Q_value.unsqueeze(1))\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb90636",
   "metadata": {},
   "source": [
    "En _Double DQN_ använder två nätverk, ett för förutsägelserna över erfarenheterna (next_Q_value ovan) och ett annat för nuvarande observationer (all_Q_values). Det tidigare nätverket lär sig i varje steg, men det andra uppdateras enligt något fördefinerat villkor genom att helt enkelt kopiera det första nätverkets vikter. Detta gör metoden mer stabil. Snart kommer vi se varför det behövs.\n",
    "\n",
    "Att implementera en DDQN utifrån koden i denna notebook är en utmärkt övning!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e603297f",
   "metadata": {},
   "source": [
    "Notera hur epsilon ändras under körningen. Detta motsvarar en _adaptive learning rate_, men minns att effekten är att agenten tar slumpmässiga handlingar med $\\epsilon$ sannoliket!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4662958d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1000, Rewards: 1000.0 "
     ]
    }
   ],
   "source": [
    "from collections import deque\n",
    "import gymnasium as gym\n",
    "\n",
    "def train_dqn(model, env, replay_buffer, optimizer, criterion, n_episodes=800,\n",
    "              warmup=30, batch_size=32, discount_factor=0.95):\n",
    "    totals = []\n",
    "    for episode in range(n_episodes):\n",
    "        epsilon = max(1 - episode / 500, 0.01)\n",
    "        seed = torch.randint(0, 2**32, size=()).item()\n",
    "        total_rewards = play_and_record_episode(model, env, replay_buffer,\n",
    "                                                epsilon, seed=seed)\n",
    "        print(f\"\\rEpisode: {episode + 1}, Rewards: {total_rewards}\", end=\" \")\n",
    "        totals.append(total_rewards)\n",
    "        if episode >= warmup:\n",
    "            dqn_training_step(model, optimizer, criterion, replay_buffer,\n",
    "                              batch_size, discount_factor)\n",
    "    return totals\n",
    "\n",
    "\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\", max_episode_steps=max_episode_steps)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "dqn = DQN()\n",
    "optimizer = torch.optim.NAdam(dqn.parameters(), lr=0.03)\n",
    "mse = nn.MSELoss()\n",
    "replay_buffer = deque(maxlen=100_000)\n",
    "totals = train_dqn(dqn, env, replay_buffer, optimizer, mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7d917f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed? False\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "dqn.eval()\n",
    "\n",
    "def dqn_policy(obs):\n",
    "    with torch.no_grad():\n",
    "        return choose_dqn_action(dqn, obs)\n",
    "\n",
    "show_one_episode(dqn_policy, seed=42)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "code",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
