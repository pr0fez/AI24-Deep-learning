{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fc4feb0",
   "metadata": {},
   "source": [
    "### Reinforcement learning\n",
    "\n",
    "Istället för att förlita sig på existerande data kan ett ML system också själv samla data och lära sig utifrån detta. I äldre litteratur kallades sådana system ofta \"självlärande\" men den användningen har fallit ur bruk. Det typiska exemplet är en robot i en omgivning (simulerad eller verklig), men RL kan användas i alla inlärningsproblem. RL blir även allt viktigare för agentisk AI. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202c5937",
   "metadata": {},
   "source": [
    "\n",
    "Reinforcement learning går ut på att sätta någon sorts poäng på utfallet av handlingar eller val i en omgivning. Detta är förstås helt enkelt ett optimeringsvillkor, men här är vi alltså intresserade av varje enskilt steg snarare än medel, summor eller minimum. Vi väljer sedan att uppdatera systemet enligt någon _policy_, som alltså är ett steg i en optimeringsalgoritm. En sådan policy kan vara vad som helst och behöver inte nödvändigtvis vara en ML algoritm. Allt policyn gör är att välja nästa handling i omgivningen. I fallet med en agent i en simulerad eller verklig omgivning är handlingarna helt enkelt interaktioner, men i ett mer abstrakt fall kan det vara val av riktningsgradient eller något annat rent matematiskt. En särksilt intressant tillämpning, både abstrakt och konkret, är _svärmintelligens_, som vi återkommer till.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d21a9b",
   "metadata": {},
   "source": [
    "<img src=\"https://kitrum.com/wp-content/uploads/2023/04/Reinforcement-Learning-1024x683.png\" height=\"320\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7c7ea3",
   "metadata": {},
   "source": [
    "\n",
    "Ett återkommande problem är hur man skall hantera tidsaspekten i RL. Belöningar i dåtid eller framtid kontra omedelbart har olika relevans för olika problem. I andra metoder hanteras detta med \"moment\", dvs någon sorts faktor som hindrar processen att fastna i minimum. Men problemet går något djupare när inlärningen sker online -- om vi tänker oss optimeringskurvan så är det någon handling som fick oss att gå ner i ett dåligt minimum. Men vilken handling ledde oss fel?\n",
    "\n",
    "I RL används en s.k. _discount factor_ för att lösa fetta. En faktor 1 innebär att systemet ger samma vikt till nuvarande som andra belöningar och \"minns\" dessa för evigt. Faktor 0 innebär att systemet ignorerar all information utom nuvarande belöning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ed4677",
   "metadata": {},
   "source": [
    "<img src=\"https://drek4537l1klr.cloudfront.net/morales/v-14/Figures/01_09.png\" height=\"480\">\n",
    "<img src=\"../Data/credit_problems.png\" height=\"480\" widht=\"640\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827c7252",
   "metadata": {},
   "source": [
    "\n",
    "Det finns en uppsjö RL tekniker, ända från datorns tidiga historia (tack vare den utförliga användingen i spelande AI). Den kanske mest kända i modern tid är från Google (2014), _Deep Q Learning_ och dess varianter. Att tillämpa just den algoritmen på schack är patenterat av Google. Den är sedan länge passerad av andra shackspelande AI. Dessa metoder har ett stort minne med tidigare handlingar och deras resultat som systemet lär sig av. En viktig del av att använda Q-learning är just att välja hur en sådan _experience replay buffer_ skall initaliseras. Erfarenheter väljs slumpvis ur buffern, till skillnad från belöningsminnet (som är seriellt). För vissa problem, som till exempel schack, är det enkelt att skapa en sådan buffer, men i många fall tar agenten slumpmässiga handlingar under en längre tid i början av träningen för att utforska tillståndsrymden. Det går bra i simulation, men skulle vara förödande i verkligheten. Det är fortfarande tidsödande och beräkningsmässigt dyrt. Andra, som den mer moderna PPO, är istället en _policy gradient_ metod, som lär sig helt online. \n",
    "\n",
    "RL system i allmänhet \"skapar sin egen träninsgdata\" live i sin omgivning. Detta innebär tyvärr att de är mycket känsliga för alla problem som annan ML har-- instabilitet under träning, känslighet för initialtillstånd och en särskild tendens att hamna i dåliga optimeringsloopar. Dessutom är de mycket mer känsliga för hyperparametrar. Det är värt att minnas att biologiska system spenderar mer resurser på _hämmning_ än på aktivering, så detta är förmodligen inte en brist i våra träningsalgoritmer eller nätverksdesign, utan ett verkligt förhållande. Realtidsinlärning är en kaotisk och parallell aktivitet där betydande resurser måste användas för att linjärisera gradienterna. Vi använder uttryck som \"reda ut\", \"få ordning på\" och så vidare när det gäller våran egen inlärning och erfarenheter, så i den lite lösa _duck-typing_ paradigmen vi använder i ML finns gott stöd för liknelsen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "c805829b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import gymnasium as gym\n",
    "max_episode_steps = 1000\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\", max_episode_steps=max_episode_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a246d79b",
   "metadata": {},
   "source": [
    "OpenAI Gymnasium har klassiska kontrollproblem, spel och simulationer. Genom tillägg finns en uppsjö mer moduler, till exempel hela katalogen av Atari 2600 spel i en förberedd emulator."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2cdd63",
   "metadata": {},
   "source": [
    "Gym tillhandahåller en stegbar miljö där varje steg ger tillbaka en ett nytt tillstånd i simulationen. Tillståndet defineras av problemet, men innehåller en observation, ett värde som anger utfallet av tidigare handling (definerat av problemet) och några tillståndsvariabler med information om problemet anses misslyckat, spelet tagit slut eller simlulationen avslutats och så vidare.\n",
    "\n",
    "I vissa fall är observationen en enkel vektor, som i CartPole exemplet -- ett klassiskt kontrollproblem som går ut på att balansera en pinne. Att flytta vagnen höger eller vänster är enda möjliga handlingarna. I andra fall är observationen en bild av ett spel, till exempel Pacman, eller något komplext tillstånd som i HumanoidStandup \n",
    "\n",
    "<img src=\"https://gymnasium.farama.org/_images/humanoid.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "3c089b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== classic_control =====\n",
      "Acrobot-v1             CartPole-v0            CartPole-v1\n",
      "MountainCar-v0         MountainCarContinuous-v0 Pendulum-v1\n",
      "===== phys2d =====\n",
      "phys2d/CartPole-v0     phys2d/CartPole-v1     phys2d/Pendulum-v0\n",
      "===== box2d =====\n",
      "BipedalWalker-v3       BipedalWalkerHardcore-v3 CarRacing-v3\n",
      "LunarLander-v3         LunarLanderContinuous-v3\n",
      "===== toy_text =====\n",
      "Blackjack-v1           CliffWalking-v1        CliffWalkingSlippery-v1\n",
      "FrozenLake-v1          FrozenLake8x8-v1       Taxi-v3\n",
      "===== tabular =====\n",
      "tabular/Blackjack-v0   tabular/CliffWalking-v0\n",
      "===== None =====\n",
      "Ant-v2                 Ant-v3                 GymV21Environment-v0\n",
      "GymV26Environment-v0   HalfCheetah-v2         HalfCheetah-v3\n",
      "Hopper-v2              Hopper-v3              Humanoid-v2\n",
      "Humanoid-v3            HumanoidStandup-v2     InvertedDoublePendulum-v2\n",
      "InvertedPendulum-v2    Pusher-v2              Reacher-v2\n",
      "Swimmer-v2             Swimmer-v3             Walker2d-v2\n",
      "Walker2d-v3\n",
      "===== mujoco =====\n",
      "Ant-v4                 Ant-v5                 HalfCheetah-v4\n",
      "HalfCheetah-v5         Hopper-v4              Hopper-v5\n",
      "Humanoid-v4            Humanoid-v5            HumanoidStandup-v4\n",
      "HumanoidStandup-v5     InvertedDoublePendulum-v4 InvertedDoublePendulum-v5\n",
      "InvertedPendulum-v4    InvertedPendulum-v5    Pusher-v4\n",
      "Pusher-v5              Reacher-v4             Reacher-v5\n",
      "Swimmer-v4             Swimmer-v5             Walker2d-v4\n",
      "Walker2d-v5\n"
     ]
    }
   ],
   "source": [
    "gym.pprint_registry()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0138daaf",
   "metadata": {},
   "source": [
    "Vi börjar med att sätta en godtycklig policy för CartPole simulationen, i varje steg vänder vi helt enkelt på riktningen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d846c1c",
   "metadata": {},
   "source": [
    "För att återställa en simulation till början använder vi <code>reset()</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "2a5efcfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raphael/code/.venv/lib64/python3.14/site-packages/gymnasium/envs/classic_control/cartpole.py:214: UserWarning: \u001b[33mWARN: You are calling 'step()' even though this environment has already returned terminated = True. You should always call 'reset()' once you receive 'terminated = True' -- any further steps are undefined behavior.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "a = 0\n",
    "done = 0\n",
    "while (not done or not trunc):\n",
    "    a = not a\n",
    "    obs, reward, done, trunc, info = env.step(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4519f858",
   "metadata": {},
   "source": [
    "Som synes av varnigen så har problemet redan misslyckats, men vi fortsätter tills _truncated_ blir sann. I detta fall är det \"max_episode_steps\" som slår till tack vare att vi skickade med den parametern i konstruktorn till <code>env</code>, men som varningen informerar oss om så är det _odefinierat_ vad som händer i allmänhet. För att stänga det öppna fönstret måste vi kalla på <code>close()</code>. Om vi tvingar ner programmet kraschar hela jupyter!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "f13ead06",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1107441",
   "metadata": {},
   "source": [
    "Om vi inte vill se agenten spela i mänsklig takt, utan vill låta datorn spela så fort som möjligt kan vi dirigera om renderingen från \"human\" till bara en bild."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "0cdfb1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\", max_episode_steps=max_episode_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0218a4b6",
   "metadata": {},
   "source": [
    "Reset ger oss en initial observation och en info variabel (tom för CartPole problemet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "ffbb81b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.0273956 , -0.00611216,  0.03585979,  0.0197368 ], dtype=float32),\n",
       " {})"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs, info = env.reset(seed=42)\n",
    "obs, info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42eef179",
   "metadata": {},
   "source": [
    "Observationen i detta fall (från dokumentationen):\n",
    "\n",
    "Cart Position\n",
    "\n",
    "Cart Velocity\n",
    "\n",
    "Pole Angle\n",
    "\n",
    "Pole Angular Velocity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a655a6",
   "metadata": {},
   "source": [
    "Om vi vill plocka ut en bild kan vi anropa <code>render()</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "66914197",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400, 600, 3)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = env.render()\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "7bdd9a60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f25a14cd090>"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f7f394",
   "metadata": {},
   "source": [
    "Som sagt finns bara höger och vänster i problemets <em>action space</em>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "1556bf75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(2)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space # 0 - vänster, 1 - höger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1d82e9",
   "metadata": {},
   "source": [
    "Låt oss pröva en lite smartare policy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "c831cc40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_policy(obs):\n",
    "    angle = obs[2]\n",
    "    if angle < 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "308bb629",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\", max_episode_steps=max_episode_steps)\n",
    "totals = []\n",
    "for episode in range(10):\n",
    "    total_rewards = 0\n",
    "    obs, info = env.reset(seed=episode)\n",
    "    trunc, done = False, False\n",
    "    while (not done and not trunc):\n",
    "        action = basic_policy(obs)\n",
    "        obs, reward, done, trunc, info = env.step(action)                     \n",
    "        total_rewards += reward        \n",
    "    totals.append(total_rewards)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22964b6",
   "metadata": {},
   "source": [
    "Som synes är problemet definerat sådant att pinnen måste vara inom en viss vinkel innan den anses ha fallit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "46de04bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(38.6), np.float64(7.472616676907761), 25.0, 51.0)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.mean(totals), np.std(totals), min(totals), max(totals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7e2be7",
   "metadata": {},
   "source": [
    "Låt oss använda ett neuralt nätverk som policy! På så sätt kan vi istället lära oss vilka inputs som är bäst för att hålla balansen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "ac0d827d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.Linear(4,5), nn.ReLU(), nn.Linear(5,1))\n",
    "\n",
    "    def forward(self, state):\n",
    "        return self.net(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ab114e",
   "metadata": {},
   "source": [
    "Vi expanderar till fem features, alltså ökar vi vår <em>latent space</em> med en dimension över <em>feature space</em>. Med ReLU mellan lagrena borde vi kunna lära oss icke-linjära interaktionseffekter mellan observationerna. Indata är fyra rella tal och utdata är ett reellt tal. Vi tolkar utdatan som en <em>logit</em> (även log-odds), alltså en sannolikhet spridd över hela $\\mathbb{R}$ $(-\\infty, +\\infty)$ istället för $[0,1]$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6835ed",
   "metadata": {},
   "source": [
    "Vi skapar sedan en Bernoulli-fördelning med den just logit spridningen. Minns att Bernoulli är en två-punkts fördelning, alltså i detta fall valet mellan höger och vänster. Sedan drar vi ett värde ur denna fördelning och ger tillbaka värdet (höger eller vänster) och hur sannolikt det var att vi drog just det värdet givet vår policy.\n",
    "\n",
    "Detta innebär att det alltid finns en chans att vi istället väljer den motsatta riktiningen istället för den nätverket tror är bäst. Litteraturen kring RL föreskriver att ett visst slumpmässigt beteende skall bevaras för att agenten skall kunna fortsätta utforska lösningsrymden och anpassa sig till förändringar. \n",
    "\n",
    "Den metod vi implementerar kallas _Monte Carlo Policy Gradient_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "dabcfa2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(model, obs):\n",
    "    state = torch.as_tensor(obs)\n",
    "    logit = model(state)\n",
    "    dist = torch.distributions.Bernoulli(logits=logit)\n",
    "    action = dist.sample()\n",
    "    log_prob = dist.log_prob(action)\n",
    "    return int(action.item()), log_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2e70d1",
   "metadata": {},
   "source": [
    "Ok! Men hur tränar vi nätverket?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b71c610",
   "metadata": {},
   "source": [
    "För att träna detta nätverk kör vi helt enkelt en episod med nuvarande policy, beräknar i efterhand vad de faktiska belöningarna med en discount factor på _framtida_ handlingar borde varit och uppdaterar vårt policy-nätverk med gradient descent på loss-funktionen (se nedan)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "c1bc2cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_returns(rewards, discount_factor):\n",
    "    returns = rewards[:]\n",
    "    for step in range(len(returns) - 1, 0, -1):\n",
    "        returns[step-1] += returns[step] * discount_factor\n",
    "    return torch.tensor(returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "098d0a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(model, env, seed=None):    \n",
    "    log_probs, rewards = [], []\n",
    "    obs, info = env.reset(seed=seed)\n",
    "    while True:\n",
    "        action, log_prob = choose_action(model, obs)\n",
    "        obs, reward, done, trunc, _ = env.step(action)\n",
    "        log_probs.append(log_prob)\n",
    "        rewards.append(reward)\n",
    "        if done or trunc:\n",
    "            return log_probs, rewards\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba72660",
   "metadata": {},
   "source": [
    "Loss-funktionen förtjänar en kommentar. \n",
    "\n",
    "\\begin{equation*}\n",
    "-\\sum{\\ln(p_x)*\\frac{x-\\bar{x}}{\\sigma}}\n",
    "\\end{equation*}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a381c61",
   "metadata": {},
   "source": [
    "Log-prob representerar sannolikheter som $(-\\infty, 0]$. Termen till höger är standardpoängen ($z$) för den justerade belöningen $x$. Standardisering rekommenderas för denna metod, men är inte strikt nödvändig. Bara $x$ går bra, men träningen blir mindre stabil. Variabeln $p_x$ är nätverkets uppskattade sannolikhet för handlingen som leder till belöningen $x$.\n",
    "\n",
    "Detta är ganska likt maximum likelihood metoden! Genom att minimera denna funktion, maximerar vi de justerade belöningarna. Strikt sett maximerar vi sannolikheten att vi väljer handlingar som tidigare lett till bra justerade belöningar. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0846d7a5",
   "metadata": {},
   "source": [
    "Vi väljer ett stoppvillkor som säger att när ett nätverk klarat balanseringsakten tre gånger i rad så är det tillräckligt bra. I online inlärning är detta ett svårt problem. Vi måste hålla reda på en sorts bästa modell men också veta när det är dags att byta modell. RL är fasansfullt instabilt utan hämmningssystem!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "73e6549a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_reinforce(model, optimizer, env, n_episodes, discount_factor):\n",
    "    model.train()\n",
    "    early_stop = list()\n",
    "    for episode in range(n_episodes):\n",
    "        seed = torch.randint(0, 2**32, size=()).item()\n",
    "        log_probs, rewards = run_episode(model, env, seed=seed)\n",
    "        returns = compute_returns(rewards, discount_factor)\n",
    "        std_returns = (returns - returns.mean()) / (returns.std() + 1e-7) # numeriskt trick för att undvika /0\n",
    "        losses = [-logp * rt for logp, rt in zip(log_probs, std_returns)]        \n",
    "        loss = torch.cat(losses).sum()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        s = sum(rewards)\n",
    "        early_stop += [s]\n",
    "        print(f\"\\rEpisode {episode + 1}, Reward: {s:.2f}\", end=\" \")\n",
    "        if all(map(lambda x: x == 1000, early_stop[-3:])):\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "e2c4d447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 148, Reward: 1000.00 "
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\", max_episode_steps=max_episode_steps)\n",
    "model = PolicyNetwork()\n",
    "optimizer = torch.optim.NAdam(model.parameters(), lr=0.06) # Nesterov moment ADAM, konvergerar snabbare (viktigt för online inlärning)\n",
    "train_reinforce(model, optimizer, env, n_episodes=1000, discount_factor=.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "e0429f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_one_episode(policy, model, seed=42):\n",
    "    model.eval()\n",
    "    frames = []\n",
    "    env = gym.make(\"CartPole-v1\", render_mode=\"human\",\n",
    "                   max_episode_steps=1000)\n",
    "    obs, info = env.reset(seed=seed)\n",
    "    rewards = []\n",
    "    while True:\n",
    "        action = policy(obs, model)\n",
    "        obs, reward, done, truncated, info = env.step(action)        \n",
    "        rewards.append(reward)\n",
    "        if done or truncated:\n",
    "            print(f\"Failed: {done}\")\n",
    "            break\n",
    "    env.close()\n",
    "    return sum(rewards)\n",
    "\n",
    "def neural_net_policy(obs, model):\n",
    "    with torch.no_grad():\n",
    "        action, _ = choose_action(model, obs)\n",
    "        return action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "138d23cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed: False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1000.0"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_one_episode(neural_net_policy, model, seed=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "code",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
