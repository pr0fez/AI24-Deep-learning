{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples of what NLP can do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* What's the topic of this text? (text classification)\n",
    "* Does this text contain abuse? (moderation)\n",
    "* Does this text sound positive or negative? (sentiment analysis)\n",
    "* What should be the next word in this incomplete sentence? (language modelling)\n",
    "* How would you say this in Dutch? (translation)\n",
    "* Produce a summary of this article in one paragraph. (summarization)\n",
    "\n",
    "# What needs to be done to process text for neural networks?\n",
    "* Standardizing; convert to lower case, remove punctuation-- although this is lossy and enables unicode injection!\n",
    "* Ignore some very common words and glyphs, e.g. \"the\", \"a/an\"-- so called _stop words_. The assumption is that these don't carry significant meaning, but some linguistics research disagrees.\n",
    "* Split the text into units (tokens), such as characters, words, groups of words, clauses in sentences, etc\n",
    "* Convert all tokens to a tensor. This means (typically) indexing the tokens.\n",
    "\n",
    "### Example\n",
    "The cat sat on the mat.\n",
    "\n",
    "the cat sat on the mat\n",
    "\n",
    "[\"cat\", \"sat\", \"on\", \"mat\"]\n",
    "\n",
    "[2, 34, 53, 8]\n",
    "\n",
    "√© -> e\n",
    "\n",
    "√® -> e\n",
    "\n",
    "but also:\n",
    "\n",
    "ƒ∏ (kappa) -> k\n",
    "\n",
    "ùëé (Mathematical Italic Small A, unicode U+1D44E) -> a\n",
    "\n",
    "These don't display in all fonts and some font-renderers don't define a glyph for missing characters, thus becoming a security concern.\n",
    "\n",
    "Then you have things like:\n",
    "\n",
    "UÃ∂ÕêÕõÃ∫Ã¶nÃµÕãÃªÃ†iÃ∏ÃçÕ†ÃóÃ¨cÃµÃêÕêÃÆÃ¨Ã•oÃ∂ÃÄÃâÕíÃÆÃºÃ¶dÃ¥ÕÉÕÑÃ±Ã∞eÃ∂ÃíÃïÃÆÕéÃ• ÃµÃíÃÇÃ≤Ã¨Ã¶iÃ∑ÕòÕùÃÇÃúsÃ∏ÕòÃ¨Ãô Ã∂ÕãÃåÃ°Ã≠Ã≤aÃ¥ÃøÃÜÃîÃ≥ ÃµÃéÕñvÃ∏ÕíÃíÃÉÕÖÃòeÃ∂ÃÅÃáÃîÃórÃ∑ÕêÃõÃ≠ÕöyÃ∑ÃêÕöÕîÃ≤ Ã∏ÃâÕëÕìÃ¶gÃ∏ÃøÕöÕáÃòeÃµÃçÕÜÃáÃ≥nÃ∂ÕíÃöÃûÕîeÃµÃâÃìÕçÃ¶Ã§rÃµÕÜÃõÃÜÃùaÃ∂ÕóÕãÃõÃ∫lÃ¥ÃöÕâÕéÃÆ Ã∏ÃâÕôtÃ¥ÃÖÃÉÃ®ÃúeÃ∑ÕùÕéÃ§xÃµÕÜÃëÕÑÕîÃªtÃ¥ÕùÃ©ÃßÃò Ã¥ÃàÃûÃÆÃÆfÃ¥ÃïÕãÃ≠Ã´oÃ¥ÕõÕ†ÃÄÃ©rÃ∑ÃîÕùÃôÕçÃümÃµÕëÃãÕÑÕéÃØÃ•aÃ¥ÃÅÃáÃ´ÃºtÃ∏ÕÑÕàÕéÃ¢\n",
    "\n",
    "\"Unicode is a very general text format\"\n",
    "\n",
    "Should the tokenizer be able to parse the text or not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Three ways of handling tokens\n",
    "## Word-level tokenization\n",
    "Tokens are space-separated substrings (or puncuation-separated if appropriate). A variant also splits into subwords, which is especially important for agglutinating and composing lanugages, such as Finnish or Swedish. \n",
    "## N-gram tokenization\n",
    "Tokens are groups of N consecutive words. For example, \"the cat\", \"he was\", \"over there\" -- these are 2-grams or \"bigrams\".\n",
    "## Character-level tokenization\n",
    "Each character is its own token. In practice, useful for languages with rich writing systems or logogrammatic writing (cyrillic, hanzi, hangeul, abjads, abugidas, devangari, etc). Some of these benefit, or even require, N-character tokenization. Others should be trained with radicals and subsets of partial characters (hangul and hanzi in particular).\n",
    "## Other tokenizations\n",
    "Also worth mentioning the linguistic concepts \"morpheme\", \"lexeme\", \"grapheme\" and \"phoneme\". You can tokenize text in several layers (embeddings) if needed!\n",
    "\n",
    "Morpheme\n",
    "> a meaningful morphological unit of a language that cannot be further divided (e.g. in, come, -ing, forming incoming )\n",
    "\n",
    "Lexeme\n",
    "> a basic lexical unit of a language consisting of one word or several words, the elements of which do not separately convey the meaning of the whole. For example 'run', 'runs', 'ran' and 'running' are all inflections of the lexeme RUN. \n",
    "\n",
    "Also consider \"take care\", \"take care of\", \"take care of the\", \"care for\", \"care for a\". These are separate _lexemes_ and the individual words \"take\", \"care\", \"of\", \"for\", \"the\", \"a\" do not separately convey the meaning of the whole lexemes; HANDLE, NURTURE, WANT, CONCERN. You can see why linguists are sceptical about stop words -- they _do_ drastically change some lexemes!\n",
    "\n",
    "Grapheme\n",
    "> the smallest meaningful contrastive unit in a writing system. English: 's', 'sh', 'ch', 'oo', 'th', 'b', 'a'\n",
    "\n",
    "Many languages written in latin alphabet(s) have complex graphemes:\n",
    "- samhailchomhartha (Irish 'symbol', celtic spelling: sa·πÅaƒ±lƒão·πÅar·π´a) \n",
    "- guillemet (French, also Norweigan, quotation marks ¬´comme √ßa¬ª)\n",
    "- przybyszewszczyzna (Polish art movement)\n",
    "\n",
    "Phoneme\n",
    "> any of the perceptually distinct units of sound in a specified language that distinguish one word from another, for example p, b, d, and t in the English words pad, pat, bad, and bat.\n",
    "\n",
    "Note that phonemes are incredibly complex. Diphtongs, ellipsis, lenition, pitch accent and tones make spoken language very difficult to generalize. For example, Swedish has tonal words despite not being a tonal language:\n",
    "- t√≤mten (a lot of land around a house)\n",
    "- t√≥mten (definite form of 'gnome', Santa Claus)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings\n",
    "\n",
    "Tokenization itself isn't enough. Tokens end up basically anywhere in token-space (the vectorscape of the vectors we defined). We can side-step this by simply attaching a linear/mlp layer to each token-vector (the one-hot encoding for each token) and _learning_ the output of the linear layer together with all the other tokens. This is called an _embedding_, specifically a _learned embedding_. The networks learn to produce outputs such that similar features are close to eachother, regardless of how far apart the tokens are in tokenspace. If we apply the same logic as we did to CNNs, we can say that similar _meanings_ or _concepts_ are clustered by the embeddings. Also not the similarity to the \"NatureCNN\" feature embeddings in PPO. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In pytorch, there's a much more efficient version; <code>nn.Embedding</code>. This bypasses the one-hot encoding entirely and thus saves alot of parameters in the model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: a traditional way to encode relevance is to use TF-IDF, Term-Frequency Inverse-Document-Frequency. This is still used in some Neural Networks. It's a statistical measure that simply counts the number of occurences of a token in one sample and divides it by the inverse occurrences of the token in all samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a many available tokenizers and embeddings. Training your own is a large undertaking and faces many particular concerns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alignment\n",
    "A famous example is that token embeddings of the English language will learn that 'King'-'Man'+'Woman' = 'Queen'. However, the same embeddings will learn 'Doctor'-'Man'+'Woman'='Nurse'. Here it has learned a pattern that can be interpreted as a historical stereotype that is not algined with current society-- especially in a country like Sweden were 58% of graduates with an MD are women. These embeddings will of course vary greatly with the corpus, what lables are present in the training data and the relevance of those features to the corpus. It may not be appropriate to learn \"all of the UFO conspiracy forums\" if the system is supposed to understand manuals for industrial machines. Similarily, deriving semantical gender or socio-economic relationships from hospital dramas or historical fiction is probably not aligned with factual society. \n",
    "\n",
    "#### Safety\n",
    "An embedding may also learn entirely unsafe relationships. A famous example from the early days of computers is the meaning of traffic-lights derived from behavioural data: \"Red - stop, Green - go, Yellow - go fast\". If these embeddings exist  already in the tokenization, the later portions of the system will have a very difficult time reintepreting the meaning. The infamous \"glue on pizza\" google recommendation is partially an embedding failure; \"food safe glue\" is not an ingredient in cooking and doesn't belong in a recipie. You glue together a broken bowl with food-safe glue. You don't eat it. Doing so is harmful. _Edible_ glue-- better known as gum arabic-- is a product used for decorative deserts but that makes little sense on a pizza...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Huggingface tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "imdb_dataset = load_dataset(\"imdb\")\n",
    "\n",
    "split = imdb_dataset[\"train\"].train_test_split(train_size=0.8)\n",
    "\n",
    "imdb_train_set, imdb_validation_set = split[\"train\"], split[\"test\"]\n",
    "\n",
    "imdb_test_set = imdb_dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'Prom Night is shot with the artistic eye someone gives while finely crafting a Lifetime original film. You know the one. This October, Lifetime takes a break from the courageous tale of a woman surviving (insert disease name here) to tell the somewhat creepy tale of a woman pursued by a stalker ex-boyfriend. It\\'s dramatic \\x85 it\\'s sappy \\x85 it\\'s immensely dull. It does nothing to further a genre, tell an original story, or strive for ANY sort of newness. Prom Night shares this plight. Watching the killer poke holes in his victims, we sit silently as they slump to the floor with not a drop of blood spilled. It occurred to me that this was the cleanest killer in movie history.<br /><br />Our director is working with a fairly good-looking killer so he is forced to pour on the camera angles to make him appear creepier. Think about Matthew McConaughey coming at you with a knife. You\\'d probably go \\x85 \"OH! Good lookin guy is going to kill me? Naaaa.\" Not scary even for a second, so the director throws Schaech into shadows and over the shoulder in the mirror. This mirror shot is repeated to the point of sickness as it practically becomes a fetish of the creator. You\\'ll get 15 jump scares in this film, 2 of which made my date jump (I might mention she is afraid of EVERYTHING). I\\'d also mention she decided to take a nap halfway through the film and at one point threatened to leave me.<br /><br />As if this film were not disjointed enough, it appears to be cut to shreds. I\\'m not saying it looks like key points were left on the cutting room floor as the crew scrambled to salvage some semblance of a horror film; I\\'m saying as the film moves from scene to scene, you often get a jarring jump. This is the kind of thing you\\'d expect when a film catches fire and a projectionist is forced to splice ends together, cross his fingers, and hope for the best. The editor should be shot.<br /><br />With a plot you can pack into two sentences, one stray spray of blood, an emo killer, and the tension of a very special episode of \"Silver Spoons\", we\\'re left with no reason to support horror this weekend \\x85 at least on the big screen. In fact, this is the sort of film that should be punished. Is it really that hard to make a scary movie? Was this crew even aware they were making a horror film??!! A complete waste of my time and yours. I bit the bullet to get you this review. Don\\'t let my sacrifice be in vain. DON\\'T GO INTO THE MOVIE!!!',\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_train_set[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tokenizers\n",
    "\n",
    "#byte-pair encoding\n",
    "bpe_model = tokenizers.models.BPE(unk_token=\"<unk>\")\n",
    "bpe_tokenizer = tokenizers.Tokenizer(bpe_model)\n",
    "bpe_tokenizer.pre_tokenizer = tokenizers.pre_tokenizers.Whitespace()\n",
    "special_tokens = [\"<pad>\", \"<unk>\"]\n",
    "bpe_trainer = tokenizers.trainers.BpeTrainer(vocab_size=1000, special_tokens=special_tokens)\n",
    "\n",
    "train_reviews = [review[\"text\"].lower() for review in imdb_train_set]\n",
    "bpe_tokenizer.train_from_iterator(train_reviews, bpe_trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoding(num_tokens=10, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_text = \"what ùëé dreadfully awesome movie!\"\n",
    "\n",
    "bpe_encoding = bpe_tokenizer.encode(my_text)\n",
    "bpe_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['what', '<unk>', 'd', 'read', 'fully', 'aw', 'es', 'ome', 'movie', '!'],\n",
       " [302, 1, 45, 574, 985, 374, 148, 223, 209, 4],\n",
       " [(0, 4),\n",
       "  (5, 6),\n",
       "  (7, 8),\n",
       "  (8, 12),\n",
       "  (12, 17),\n",
       "  (18, 20),\n",
       "  (20, 22),\n",
       "  (22, 25),\n",
       "  (26, 31),\n",
       "  (31, 32)])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpe_encoding.tokens, bpe_encoding.ids, bpe_encoding.offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Encoding(num_tokens=377, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]),\n",
       " Encoding(num_tokens=803, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]),\n",
       " Encoding(num_tokens=627, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpe_tokenizer.encode_batch(train_reviews[:3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpe_tokenizer.enable_padding(pad_id=0, pad_token=\"<pad>\")\n",
    "bpe_tokenizer.enable_truncation(max_length=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "bpe_encodings = bpe_tokenizer.encode_batch_fast(train_reviews[:3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Encoding(num_tokens=500, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]),\n",
       " Encoding(num_tokens=500, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]),\n",
       " Encoding(num_tokens=500, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpe_encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[509,  10,  60,  ...,   0,   0,   0],\n",
       "        [394, 164, 817,  ..., 150, 980, 274],\n",
       "        [167, 323,  55,  ..., 317, 189, 439]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpe_batch_ids = torch.tensor([encoding.ids for encoding in bpe_encodings])\n",
    "bpe_batch_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask = torch.tensor([encoding.attention_mask for encoding in bpe_encodings])\n",
    "attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([377, 500, 500])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths = attention_mask.sum(dim=-1)\n",
    "lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Byte-level Byte-pair Encoding (BPPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "gpt2_tokenizer = transformers.AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "gpt2_encoding = gpt2_tokenizer(train_reviews[:3], truncation=True, max_length=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1456, 338, 262, 922, 1705, 717, 13, 366, 38685, 1]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt2_token_ids = gpt2_encoding[\"input_ids\"][0][:10]\n",
    "gpt2_token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'here\\'s the good news first. \"spirit\"'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt2_tokenizer.decode(gpt2_token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* BPPE\n",
    "  > GPT, Llama, RoBERTa, BLOOM\n",
    "* WordPiece\n",
    "  > BERT, DistillBERT, ELECTRA\n",
    "* Unigram\n",
    "  > ALBERT, mBART, h√†nz√¨, hangeul\n",
    "* SentencePiece\n",
    "  > Subword tokenization, e.g. Arabic, Finnish, German, Hungarian, Polish, Swedish, Turkish\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are also pre-trained embeddings: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = transformers.AutoModel.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(30522, 768, padding_idx=0)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_model.embeddings.word_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the pytorch embedding layer; this model was implemented in pytorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'most famous' embeddings are:\n",
    "* word2vec (Google)\n",
    "* GloVe (Stanford)\n",
    "* FastText (Facebook/Meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional encodings\n",
    "\n",
    "Natural language is full of ordered information, but unlike a computer system they aren't in nice structures like stacks, trees or lists. An obvious solution for an NN is to simply add an encoding of _position_ within a text and attach an embedding as with the tokens themselves (ie a sparse vector). See pp 584-585 in the book, and the corresponding example in the handson-mlp repo for an example implementation. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "code",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
