{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6722750e",
   "metadata": {},
   "source": [
    "## Gradienter, backpropagation\n",
    "\n",
    "Gradienterna i djupa nätverk är inte stabila vid numerisk derivering (beräkning av gradienterna). Två huvudsakliga beteenden observeras:\n",
    "* Försvinnande gradient (Vanishing Gradient) -- gradienten går mot noll desto djupare i närverket algoritmen går. Alltså slutar nätverket lära sig och de djupa lagren slutar uppdateras. Nätverket konvergerar inte mot någon lösning.\n",
    "* Exploderande gradient (Exploding Gradient) -- gradienten divergerar och därmed även den förutsagda lösningen. Dessa gradienter kan till och med vara periodiska eller kaotiska. Förekommer i allmänhet bara i _rekurrenta_ nätverk (RNN), även om undantag finns!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86c92d9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Vad som händer matematiskt\n",
    "\n",
    "Detta var, som så mycket annat, en empirisk upptäckt. Ingen matematik fanns som förklarade beteendet\n",
    "och ledde till att DNN (djupa neurala nätverk) övergavs kring 2000. Xavier Glorot och Yoshua Bengio \n",
    "visade kring 2010 att kombinationen av den aktiveringsfunktion som var mest populär (sigmoid) och \n",
    "hur vikterna i nätverket initialiserades ledde till att variansen ökade i varje lager. Det var alltså\n",
    "en kombination av en statistisk effekt och ett numerisk närmevärde.\n",
    "\n",
    "<img src=\"../Data/sigmoid_sat.jpg\" />\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf199f2d",
   "metadata": {},
   "source": [
    "I allmänhet kan inte variansen för både gradienterna och grundsanningen hållas nere om inte alla lager har samma antal kopplingar in och ut, vilket är en ytterligare anledning varför vissa nätverk föredrar att hålla samma antal noder i alla lager. Men Glorot och Bengio hittade en bra kompromiss, som visar sig fungera bra nog i praktiken: så-kallad _Glorot initialisering_.\n",
    "\n",
    "Men den tekniken gäller bara för sigmoida funktioner. För ReLU föredras _He-initialisering_ (även _Kaiming-initialisering_ efter forskaren Kaiming He). Dessa skiljer sig mest i skalfaktorer och vilken statistika som används för variansen (in, ut eller medel). En till vanlig initalisering är _LeCun_ initialisering, som oftast används med SELU men även just nätverk där antalet noder i alla lager är detsamma.\n",
    "\n",
    "<img src=\"../Data/active_init.jpg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8fc19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "nn.init.kaiming_uniform_(layer.weight)\n",
    "nn.init.zeros_(layer.bias)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
